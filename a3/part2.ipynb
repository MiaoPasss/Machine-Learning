{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def reduce_logsumexp(input_tensor, reduction_indices=1, keep_dims=False):\n",
    "    \"\"\"Computes the sum of elements across dimensions of a tensor in log domain.\n",
    "\n",
    "     It uses a similar API to tf.reduce_sum.\n",
    "\n",
    "    Args:\n",
    "    input_tensor: The tensor to reduce. Should have numeric type.\n",
    "    reduction_indices: The dimensions to reduce. \n",
    "    keep_dims: If true, retains reduced dimensions with length 1.\n",
    "    Returns:\n",
    "    The reduced tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_input_tensor1 = tf.reduce_max(\n",
    "        input_tensor, reduction_indices, keep_dims=keep_dims)\n",
    "    max_input_tensor2 = max_input_tensor1\n",
    "    \n",
    "    if not keep_dims:\n",
    "        max_input_tensor2 = tf.expand_dims(max_input_tensor2, reduction_indices)\n",
    "        \n",
    "    return tf.log(\n",
    "        tf.reduce_sum(\n",
    "            tf.exp(input_tensor - max_input_tensor2),\n",
    "            reduction_indices,\n",
    "            keep_dims=keep_dims)) + max_input_tensor1\n",
    "\n",
    "\n",
    "def logsoftmax(input_tensor):\n",
    "    \"\"\"Computes normal softmax nonlinearity in log domain.\n",
    "\n",
    "     It can be used to normalize log probability.\n",
    "     The softmax is always computed along the second dimension of the input Tensor.     \n",
    "\n",
    "    Args:\n",
    "    input_tensor: Unnormalized log probability.\n",
    "    Returns:\n",
    "    normalized log probability.\n",
    "    \"\"\"\n",
    "    return input_tensor - reduce_logsumexp(input_tensor, reduction_indices=0, keep_dims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper as hlp\n",
    "\n",
    "# Loading data\n",
    "#data_in = np.load('data2D.npy')\n",
    "data_in = np.load('data100D.npy')\n",
    "[num_pts, dim] = np.shape(data_in)\n",
    "\n",
    "N = num_pts\n",
    "D = dim\n",
    "\n",
    "# Distance function for K-means\n",
    "def distanceFunc(X, MU):\n",
    "    # Inputs\n",
    "    # X: is an NxD matrix (N observations and D dimensions)\n",
    "    # MU: is an KxD matrix (K means and D dimensions)\n",
    "    # Outputs\n",
    "    # pair_dist: is the squared pairwise distance matrix (NxK)\n",
    "    # TODO\n",
    "\n",
    "    return tf.reduce_sum(tf.squared_difference(tf.expand_dims(X,1),tf.expand_dims(MU,0)), 2)\n",
    "\n",
    "def K_Means(k, is_valid=False):\n",
    "    # For Validation set\n",
    "    if is_valid:\n",
    "        valid_batch = int(num_pts / 3.0)\n",
    "        train_batch = N - valid_batch\n",
    "        np.random.seed(45689)\n",
    "        rnd_idx = np.arange(num_pts)\n",
    "        np.random.shuffle(rnd_idx)\n",
    "        val_data = data_in[rnd_idx[:valid_batch]]\n",
    "        data = data_in[rnd_idx[valid_batch:]]\n",
    "    else:\n",
    "        data = data_in\n",
    "\n",
    "    MU = tf.Variable(tf.random_normal([k, D]))\n",
    "    X = tf.placeholder(tf.float32, shape = [None, D])\n",
    "\n",
    "    dist_matrix = distanceFunc(X, MU)\n",
    "    dist = tf.reduce_min(dist_matrix, axis = 1)\n",
    "    cluster = tf.argmin(dist_matrix, axis = 1)\n",
    "    _, _, count = tf.unique_with_counts(cluster)\n",
    "    percentage = tf.divide(count,N)\n",
    "\n",
    "    loss_mu = tf.reduce_sum(dist)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1=0.9, beta2=0.99, epsilon=1e-5)\n",
    "    optimizer = optimizer.minimize(loss_mu)\n",
    "\n",
    "    train_record = []\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.InteractiveSession()\n",
    "    session.run(init)\n",
    "\n",
    "    for i in range(200):\n",
    "        _, loss = session.run([optimizer, loss_mu], feed_dict = {X: data})\n",
    "        train_record.append(loss)\n",
    "\n",
    "    clusters, percentages = session.run([cluster, percentage], feed_dict = {X: data})\n",
    "\n",
    "    if is_valid is True:\n",
    "        valid_loss = session.run([loss_mu], feed_dict = {X: val_data})\n",
    "\n",
    "        session.close()\n",
    "        return valid_loss\n",
    "\n",
    "    else:\n",
    "        plt.figure()\n",
    "        plt.title('Loss with number of clusters = {}'.format(k))\n",
    "        plt.plot(train_record,'')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.grid()\n",
    "        plt.legend(['Training Loss'])\n",
    "        plt.savefig('Training_Loss_k={}.png'.format(k))\n",
    "\n",
    "        print(percentages)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Data Points with number of clusters = {}'.format(k))\n",
    "        plt.scatter(data_in[:,0], data_in[:,1], c=clusters, s=0.5, alpha=0.8)\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        plt.grid()\n",
    "        plt.savefig('Data_Points_k={}.png'.format(k))\n",
    "\n",
    "\n",
    "        session.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import helper as hlp\n",
    "\n",
    "# Loading data\n",
    "data = np.load('data100D.npy')\n",
    "#data = np.load('data2D.npy')\n",
    "[num_pts, dim] = np.shape(data)\n",
    "\n",
    "N = num_pts\n",
    "D = dim\n",
    "\n",
    "def MoG(k, is_valid=False):\n",
    "    # For Validation set\n",
    "    train_batch = N\n",
    "    train_data = data\n",
    "    if is_valid:\n",
    "        valid_batch = int(num_pts / 3.0)\n",
    "        train_batch = N - valid_batch\n",
    "        np.random.seed(45689)\n",
    "        rnd_idx = np.arange(num_pts)\n",
    "        np.random.shuffle(rnd_idx)\n",
    "        val_data = data[rnd_idx[:valid_batch]]\n",
    "        train_data = data[rnd_idx[valid_batch:]]\n",
    "\n",
    "    Mu = tf.Variable(tf.random.truncated_normal([k, D]))\n",
    "    X = tf.placeholder(tf.float32, shape = [None, D])\n",
    "    phi = tf.Variable(tf.random.truncated_normal([k, 1]))\n",
    "    psi = tf.Variable(tf.random.truncated_normal([k, 1]))\n",
    "    sigma = tf.exp(phi)\n",
    "    log_pi = logsoftmax(psi)\n",
    "\n",
    "    log_Gauss = log_GaussPDF(X,Mu,sigma)\n",
    "    log_pstr = log_posterior(log_Gauss, log_pi)\n",
    "\n",
    "    mle_predict = tf.argmax(log_pstr,axis=1)\n",
    "    logloss = - tf.reduce_sum(reduce_logsumexp(log_Gauss + tf.transpose(log_pi)),axis=0)\n",
    "    _, _, count = tf.unique_with_counts(mle_predict)\n",
    "    percentage = tf.divide(count, train_batch)\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.1, beta1=0.9, beta2=0.99, epsilon=1e-5)\n",
    "    optimizer = optimizer.minimize(logloss)\n",
    "\n",
    "    train_record = []\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.InteractiveSession()\n",
    "    session.run(init)\n",
    "\n",
    "    for i in range(200):\n",
    "        _, loss = session.run([optimizer, logloss], feed_dict = {X: train_data})\n",
    "        train_record.append(loss)\n",
    "\n",
    "    predict, percentages = session.run([mle_predict, percentage], feed_dict = {X: train_data})\n",
    "\n",
    "    if is_valid is True:\n",
    "        valid_loss = session.run(logloss, feed_dict = {X: val_data})\n",
    "        session.close()\n",
    "        return valid_loss\n",
    "\n",
    "    else:\n",
    "    \n",
    "        plt.figure()\n",
    "        plt.title('Loss with number of gaussian clusters = {}'.format(k))\n",
    "        plt.plot(train_record,'')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.grid()\n",
    "        plt.legend(['Training Loss'])\n",
    "        plt.savefig('Gaussian_Training_Loss_k={}.png'.format(k))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Data Points with number of gaussian clusters = {}'.format(k))\n",
    "        plt.scatter(data[:,0], data[:,1], c=predict, s=0.5, alpha=0.8)\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        plt.grid()\n",
    "        plt.savefig('Gaussian_Data_Points_k={}.png'.format(k))\n",
    "        \n",
    "\n",
    "        print(\"Final value of phi and psi:\")\n",
    "        print(phi.eval())\n",
    "        print(psi.eval())\n",
    "\n",
    "        print(\"Final value of sigma and pi:\")\n",
    "        print(sigma.eval())\n",
    "        print(tf.exp(log_pi).eval())\n",
    "\n",
    "\n",
    "        session.close()\n",
    "        return\n",
    "\n",
    "\n",
    "# Distance function for GMM\n",
    "def distanceFunc(X, MU):\n",
    "    # Inputs\n",
    "    # X: is an NxD matrix (N observations and D dimensions)\n",
    "    # MU: is an KxD matrix (K means and D dimensions)\n",
    "    # Outputs\n",
    "    # pair_dist: is the pairwise distance matrix (NxK)\n",
    "    # TODO\n",
    "    return tf.reduce_sum(tf.squared_difference(tf.expand_dims(X,1),tf.expand_dims(MU,0)), 2)\n",
    "\n",
    "def log_GaussPDF(X, mu, sigma):\n",
    "    # Inputs\n",
    "    # X: N X D\n",
    "    # mu: K X D\n",
    "    # sigma: K X 1\n",
    "\n",
    "    # Outputs:\n",
    "    # log Gaussian PDF N X K\n",
    "\n",
    "    distance = distanceFunc(X,mu)\n",
    "    exp = - tf.divide(distance, 2 * tf.transpose(sigma))\n",
    "    coef = - (D / 2) * tf.log(2 * np.pi) - (1/2) * tf.log(tf.transpose(sigma))\n",
    "    return tf.add(coef, exp)\n",
    "\n",
    "\n",
    "\n",
    "def log_posterior(log_PDF, log_pi):\n",
    "    # Input\n",
    "    # log_PDF: log Gaussian PDF N X K\n",
    "    # log_pi: K X 1\n",
    "\n",
    "    # Outputs\n",
    "    # log_post: N X K\n",
    "\n",
    "    # TODO\n",
    "    num = tf.add(log_PDF, tf.transpose(log_pi))\n",
    "    den = reduce_logsumexp(num, keep_dims=True)\n",
    "    return tf.subtract(num,den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means:\n",
      "k = 5, valid loss =  [71794.9]\n",
      "k = 10, valid loss =  [152839.23]\n",
      "k = 15, valid loss =  [70407.336]\n",
      "k = 20, valid loss =  [69752.516]\n",
      "k = 30, valid loss =  [68940.375]\n",
      "WARNING:tensorflow:From <ipython-input-1-39f0a613d7d9>:18: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-1-39f0a613d7d9>:28: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "MoG:\n",
      "k = 5, valid loss =  315629.84\n",
      "k = 10, valid loss =  315623.9\n",
      "k = 15, valid loss =  315639.34\n",
      "k = 20, valid loss =  315623.5\n",
      "k = 30, valid loss =  315624.03\n"
     ]
    }
   ],
   "source": [
    "print('K-Means:')\n",
    "print('k = 5, valid loss = ', K_Means(5, True))\n",
    "print('k = 10, valid loss = ', K_Means(10, True))\n",
    "print('k = 15, valid loss = ', K_Means(15, True))\n",
    "print('k = 20, valid loss = ', K_Means(20, True))\n",
    "print('k = 30, valid loss = ', K_Means(30, True))\n",
    "\n",
    "\n",
    "print('MoG:\\nk = 5, valid loss = ', MoG(5, True))\n",
    "print('k = 10, valid loss = ', MoG(10, True))\n",
    "print('k = 15, valid loss = ', MoG(15, True))\n",
    "print('k = 20, valid loss = ', MoG(20, True))\n",
    "print('k = 30, valid loss = ', MoG(30, True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
