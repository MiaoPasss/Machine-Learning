{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FTj_VwcojVF",
        "colab_type": "text"
      },
      "source": [
        "# __<center>ECE421 - Winter 2020__\n",
        "# __<center> Assignment 1:__\n",
        "# **<center>Linear and Logistic Regression**\n",
        "\n",
        "Haotian Luo 1004102865 <br>\n",
        "Yuheng Wang\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SkBUh0x3NUi",
        "colab_type": "text"
      },
      "source": [
        "## 1. Linear Regression\n",
        "### 1.1 Loss Function and Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqCGCJ7n1n8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData():\n",
        "    with np.load('notMNIST.npz') as data :\n",
        "        Data, Target = data ['images'], data['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n",
        "\n",
        "def MSE(W, b, x, y, reg):\n",
        "    y_hat = np.matmul(x, W) + b\n",
        "    mse_loss = (np.linalg.norm(y_hat - y)) ** 2\n",
        "    wd_loss = (np.linalg.norm(W) ** 2) * reg / 2\n",
        "\n",
        "    return (mse_loss + wd_loss)\n",
        "    \n",
        "def gradMSE(W, b, x, y, reg):\n",
        "    N = len(y)\n",
        "    y_hat = np.matmul(x, W) + b\n",
        "    grad_weight = (1/N) * np.dot(np.transpose(x), np.subtract(y_hat, y)) + reg * W\n",
        "    grad_bias = (1/N) * np.sum(y_hat - y)\n",
        "\n",
        "    return grad_weight, grad_bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pEpeKs1nLT",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl4eVYMt8-nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def timer(method):\n",
        "#    def timed(*args, **kwargs):\n",
        "#        ts = time.time()\n",
        "#        result = method(*args, **kwargs)\n",
        "#        te = time.time()\n",
        "#        print (method.__name__, ':', (te - ts) * 1000, 'ms')\n",
        "#        return result\n",
        "#    return timed\n",
        "\n",
        "#@timer\n",
        "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol, lossType = \"MSE\"):\n",
        "    weight_record = []\n",
        "    bias_record = []\n",
        "    loss_record = []\n",
        "\n",
        "    print(\"Calculating gradient descent of\", lossType, \"with learning rate =\", alpha, \"and regularizer =\", reg)\n",
        "    current_weight = W.reshape(x.shape[1], 1)\n",
        "    current_bias = b\n",
        "    \n",
        "    if lossType is \"MSE\":\n",
        "        for _ in range(0, epochs):\n",
        "            grad_w, grad_b = gradMSE(current_weight, current_bias, x, y, reg)\n",
        "            updated_weight = current_weight - np.multiply(alpha, grad_w)\n",
        "            if (np.linalg.norm(updated_weight - current_weight) < error_tol):\n",
        "                break\n",
        "            updated_bias = current_bias - np.multiply(alpha, grad_b)\n",
        "            loss = MSE(updated_weight, updated_bias, x, y, reg)\n",
        "            current_weight = updated_weight\n",
        "            current_bias = updated_bias\n",
        "            weight_record.append(updated_weight)\n",
        "            bias_record.append(updated_bias)\n",
        "            loss_record.append(loss)\n",
        "    elif lossType is \"CE\":\n",
        "        for _ in range(0, epochs):\n",
        "            grad_w, grad_b = gradCE(current_weight, current_bias, x, y, reg)\n",
        "            updated_weight = current_weight - np.multiply(alpha, grad_w)\n",
        "            if (np.linalg.norm(updated_weight - current_weight) < error_tol):\n",
        "                break\n",
        "            updated_bias = current_bias - np.multiply(alpha, grad_b)\n",
        "            loss = crossEntropyLoss(updated_weight, updated_bias, x, y, reg)\n",
        "            current_weight = updated_weight\n",
        "            current_bias = updated_bias\n",
        "            weight_record.append(updated_weight)\n",
        "            bias_record.append(updated_bias)\n",
        "            loss_record.append(loss)\n",
        "\n",
        "    else:\n",
        "        exit -1\n",
        "    return weight_record, bias_record, loss_record\n",
        "\n",
        "    \n",
        "def crossEntropyLoss(W, b, x, y, reg):\n",
        "    N,n = x.shape\n",
        "    y_hat = 1./(1 + np.exp(-(np.dot(x,W) + b)))\n",
        "    total_loss = 1/N * (np.sum(-np.multiply(y, np.log(y_hat) - np.multiply((1 - y), np.log(y_hat))))) + reg/2 * (np.linalg.norm(W) ** 2)\n",
        "    return total_loss\n",
        "\n",
        "def gradCE(W, b, x, y, reg):\n",
        "    N,n = x.shape\n",
        "    y_hat = 1./(1 + np.exp(-(np.dot(x,W) + b)))\n",
        "    grad_weight = 1/N * np.dot(x.T, y - y_hat) + reg * np.linalg.norm(W)\n",
        "    grad_bias = 1/N * np.sum(y - y_hat)\n",
        "    return grad_weight, grad_bias\n",
        "\n",
        "def accuracy_calculation(W, b, x, y):\n",
        "    acc = [np.sum((np.dot(x, W[i]) + b[i] >= 0.5) == y) / y.shape[0] for i in range(len(W))]\n",
        "    return acc\n",
        "    \n",
        "def buildGraph(loss=\"MSE\"):\n",
        "    #Initialize weight and bias tensors\n",
        "    tf.set_random_seed(421)\n",
        "\n",
        "    if loss == \"MSE\":\n",
        "    # Your implementation\n",
        "        return\n",
        "\n",
        "    elif loss == \"CE\":\n",
        "    #Your implementation here\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYspYWfDekG2",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Tuning the Learning Rate\n",
        "\n",
        "In the below chart, data is trained with Batch Gradient Descent Algorithm. We fix the regularizer $\\lambda$ = 0, and we change the learning rate $\\alpha$ = {0.005, 0.001, 0.0001}. The final *Mean Square Error, Trainning Accuracy, Validation Accuracy, Testing Accuracy, and Computation Time* are measured and recorded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLe0mqSpev6L",
        "colab_type": "text"
      },
      "source": [
        "| $\\alpha$ = |MSE|Training Accuracy|Validation Accuracy|Testing Accuracy|Computation Time|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "|0.005|7072.130|0.644|0.61|0.634|16390 ms|\n",
        "|0.001|23569.585|0.572|0.56|0.476|15685 ms|\n",
        "|0.0001|77365.574|0.527|0.54|0.524|15843 ms|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNVo5V3lj4Mv",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Generalization\n",
        "\n",
        "In the below chart, data is trained with Batch Gradient Descent Algorithm. We fix the learning rate $\\alpha$ = 0.005, and we change the regularizer $\\lambda$ = {0.001, 0.1, 0.5}. The final *Mean Square Error, Trainning Accuracy, Validation Accuracy, Testing Accuracy, and Computation Time* are measured and recorded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZZs56grk_wu",
        "colab_type": "text"
      },
      "source": [
        "| $\\lambda$ = |MSE|Training Accuracy|Validation Accuracy|Testing Accuracy|Computation Time|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "|0.001|6733.505|0.648|0.61|0.634|15726 ms|\n",
        "|0.1|158.814|0.962|0.97|0.966|15416 ms|\n",
        "|0.5|121.288|0.968|0.97|0.966|15581 ms|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZtj2psUnjqG",
        "colab_type": "text"
      },
      "source": [
        "### 1.5 Comparing Batch GD with normal equation\n",
        "\n",
        "In the below chart, we calculate the optimum weight using \"normal equation\" of the **least sqaures** formula. \n",
        "$$ \\underline{W}_{LS} = (X^TX + \\lambda I)^{-1} X^T Y $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yztWMxrR9CXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_equation (x, y, reg):\n",
        "    data_size = x.shape[0]\n",
        "    dimension = x.shape[1]\n",
        "    x = np.append(np.ones((data_size, 1)), x, axis = 1)\n",
        "\n",
        "    I = np.identity(dimension + 1)\n",
        "    I[:, 0] = 0\n",
        "\n",
        "    w_normal = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(x), x) + reg * I), np.transpose(x)), y)\n",
        "    b_normal = w_normal[0]\n",
        "    w_normal = np.delete(w_normal, 0).reshape(dimension, 1)\n",
        "    \n",
        "    return w_normal, b_normal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1qeg9c2SyE",
        "colab_type": "text"
      },
      "source": [
        "We change the regularizer  λ  = {0, 0.001, 0.1, 0.5}. The final *Mean Square Error, Trainning Accuracy, Validation Accuracy, Testing Accuracy, and Computation Time* are measured and recorded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIqXHaRWur1F",
        "colab_type": "text"
      },
      "source": [
        "| $\\lambda$ = |MSE|Training Accuracy|Validation Accuracy|Testing Accuracy|Computation Time|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "|0|65.464|0.994|0.96|0.945|340 ms|\n",
        "|0.001|65.470|0.994|0.96|0.945|299 ms|\n",
        "|0.1|158.814|0.994|0.97|0.945|299 ms|\n",
        "|0.5|121.288|0.993|0.97|0.945|300 ms|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICsTiAa9wCr6",
        "colab_type": "text"
      },
      "source": [
        "Then, we compare the loss, accuracy, and computation time between **Batch GD Algorithm** and **Normal Equation**. The learning rate $\\alpha$ of Batch GD is chosen to be 0.005 in each case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WnPBL7ow8Fl",
        "colab_type": "text"
      },
      "source": [
        "| $\\lambda$ = |Method: B/N|MSE|Training Accuracy|Validation Accuracy|Testing Accuracy|Computation Time|\n",
        "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
        "|0|Batch GD|7072.130|0.644|0.61|0.634|16390 ms|\n",
        "||Normal Eq|65.464|0.994|0.96|0.945|340 ms|\n",
        "|0.001|Batch GD|6733.505|0.648|0.61|0.634|15726 ms|\n",
        "||Normal Eq|65.470|0.994|0.96|0.945|299 ms|\n",
        "|0.1|Batch GD|158.814|0.962|0.97|0.966|15416 ms|\n",
        "||Normal Eq|158.814|0.994|0.97|0.945|299 ms|\n",
        "|0.5|Batch GD|121.288|0.968|0.97|0.966|15581 ms|\n",
        "||Normal Eq|121.288|0.993|0.97|0.945|300 ms|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOsrWXuwIR0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def print_info(loss, train_a, valid_a, test_a, type, alpha, reg, comp_time):    \n",
        "    if type is \"GD\":\n",
        "        print('Batch GD with \\u03B1 = {}, \\u03BB = {}, MSE = {}, training accuracy = {}, valid accuracy = {}, test accuracy = {}, '\n",
        "            'computation time = {} ms'.format(alpha, reg, loss, train_a, valid_a, test_a, int(comp_time * 1000)))\n",
        "    elif type is \"normal\":\n",
        "        print('Normal Equation with \\u03BB = {}, MSE = {}, training accuracy = {}, valid accuracy = {}, test accuracy = {}, '\n",
        "            'computation time = {} ms'.format(reg, loss, train_a, valid_a, test_a, int(comp_time * 1000)))\n",
        "\n",
        "def linreg():\n",
        "    #1.3\n",
        "\n",
        "    import time\n",
        "\n",
        "    trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "    trainData = np.array([x.flatten() for x in trainData])\n",
        "    validData = np.array([x.flatten() for x in validData])\n",
        "    testData = np.array([x.flatten() for x in testData])\n",
        "\n",
        "\n",
        "    alpha1, alpha2, alpha3 = 0.005, 0.001, 0.0001\n",
        "    reg = 0\n",
        "    epochs = 5000\n",
        "    error_tol = 1e-7\n",
        "\n",
        "    init_weight = np.random.normal(size=(784,1))\n",
        "    init_bias = np.random.uniform(-1,1)\n",
        "\n",
        "    start1 = time.time()\n",
        "    weight_train1, bias_train1, loss_train1 = grad_descent(init_weight, init_bias, trainData, trainTarget, alpha1, epochs, reg, error_tol)\n",
        "    end1 = time.time()\n",
        "    start2 = time.time()\n",
        "    weight_train2, bias_train2, loss_train2 = grad_descent(init_weight, init_bias, trainData, trainTarget, alpha2, epochs, reg, error_tol)\n",
        "    end2 = time.time()\n",
        "    start3 = time.time()\n",
        "    weight_train3, bias_train3, loss_train3 = grad_descent(init_weight, init_bias, trainData, trainTarget, alpha3, epochs, reg, error_tol)\n",
        "    end3 = time.time()\n",
        "\n",
        "\n",
        "    accuracy_train1 = accuracy_calculation(weight_train1, bias_train1, trainData, trainTarget)\n",
        "    accuracy_train2 = accuracy_calculation(weight_train2, bias_train2, trainData, trainTarget)\n",
        "    accuracy_train3 = accuracy_calculation(weight_train3, bias_train3, trainData, trainTarget)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Training losses')\n",
        "    plt.plot(loss_train1,'',loss_train2,'',loss_train3,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('losses')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha1, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha2, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha3, reg)])\n",
        "    plt.savefig('Learning_rate_adjustment_training_loss_LinReg.png')\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Training accuracy')\n",
        "    plt.plot(accuracy_train1,'',accuracy_train2,'',accuracy_train3,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha1, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha2, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha3, reg)])\n",
        "    plt.savefig('Learning_rate_adjustment_training_accuracy_LinReg.png')\n",
        "\n",
        "    accuracy_valid1 = accuracy_calculation(weight_train1, bias_train1, validData, validTarget)\n",
        "    accuracy_valid2 = accuracy_calculation(weight_train2, bias_train2, validData, validTarget)\n",
        "    accuracy_valid3 = accuracy_calculation(weight_train3, bias_train3, validData, validTarget)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Validation accuracy')\n",
        "    plt.plot(accuracy_valid1,'',accuracy_valid2,'',accuracy_valid3,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha1, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha2, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha3, reg)])\n",
        "    plt.savefig('Learning_rate_adjustment_validation_accuracy_LinReg.png')\n",
        "\n",
        "    accuracy_test1 = accuracy_calculation(weight_train1, bias_train1, testData, testTarget)\n",
        "    accuracy_test2 = accuracy_calculation(weight_train2, bias_train2, testData, testTarget)\n",
        "    accuracy_test3 = accuracy_calculation(weight_train3, bias_train3, testData, testTarget)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Testing accuracy')\n",
        "    plt.plot(accuracy_test1,'',accuracy_test2,'',accuracy_test3,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha1, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha2, reg),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha3, reg)])\n",
        "    plt.savefig('Learning_rate_adjustment_testing_accuracy_LinReg.png')\n",
        "\n",
        "    print_info(loss_train1[-1], accuracy_train1[-1], accuracy_valid1[-1], accuracy_test1[-1], \"GD\", alpha1, reg, end1 - start1)\n",
        "    print_info(loss_train2[-1], accuracy_train2[-1], accuracy_valid2[-1], accuracy_test2[-1], \"GD\", alpha2, reg, end2 - start2)\n",
        "    print_info(loss_train3[-1], accuracy_train3[-1], accuracy_valid3[-1], accuracy_test3[-1], \"GD\", alpha3, reg, end3 - start3)\n",
        "\n",
        "\n",
        "    #1.4\n",
        "    alpha = 0.005\n",
        "    reg1, reg2, reg3 = 0.001, 0.1, 0.5\n",
        "    \n",
        "    start4 = time.time()\n",
        "    weight_train4, bias_train4, loss_train4 = grad_descent(init_weight, init_bias, trainData, trainTarget, alpha, epochs, reg1, error_tol)\n",
        "    end4 = time.time()\n",
        "    start5 = time.time()\n",
        "    weight_train5, bias_train5, loss_train5 = grad_descent(init_weight, init_bias, trainData, trainTarget, alpha, epochs, reg2, error_tol)\n",
        "    end5 = time.time()\n",
        "    start6 = time.time()\n",
        "    weight_train6, bias_train6, loss_train6 = grad_descent(init_weight, init_bias, trainData, trainTarget, alpha, epochs, reg3, error_tol)\n",
        "    end6 = time.time()\n",
        "\n",
        "\n",
        "    accuracy_train4 = accuracy_calculation(weight_train4, bias_train4, trainData, trainTarget)\n",
        "    accuracy_train5 = accuracy_calculation(weight_train5, bias_train5, trainData, trainTarget)\n",
        "    accuracy_train6 = accuracy_calculation(weight_train6, bias_train6, trainData, trainTarget)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Training losses')\n",
        "    plt.plot(loss_train4,'',loss_train5,'',loss_train6,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('losses')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg1),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg2),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg3)])\n",
        "    plt.savefig('Regulation_adjustment_training_loss_LinReg.png')\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Training accuracy')\n",
        "    plt.plot(accuracy_train4,'',accuracy_train5,'',accuracy_train6,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg1),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg2),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg3)])\n",
        "    plt.savefig('Regulation_adjustment_training_accuracy_LinReg.png')\n",
        "\n",
        "    accuracy_valid4 = accuracy_calculation(weight_train4, bias_train4, validData, validTarget)\n",
        "    accuracy_valid5 = accuracy_calculation(weight_train5, bias_train5, validData, validTarget)\n",
        "    accuracy_valid6 = accuracy_calculation(weight_train6, bias_train6, validData, validTarget)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Validation accuracy')\n",
        "    plt.plot(accuracy_valid4,'',accuracy_valid5,'',accuracy_valid6,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg1),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg2),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg3)])\n",
        "    plt.savefig('Regulation_adjustment_validation_accuracy_LinReg.png')\n",
        "\n",
        "    accuracy_test4 = accuracy_calculation(weight_train4, bias_train4, testData, testTarget)\n",
        "    accuracy_test5 = accuracy_calculation(weight_train5, bias_train5, testData, testTarget)\n",
        "    accuracy_test6 = accuracy_calculation(weight_train6, bias_train6, testData, testTarget)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.suptitle('Testing accuracy')\n",
        "    plt.plot(accuracy_test4,'',accuracy_test5,'',accuracy_test6,'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.grid()\n",
        "    plt.legend(['MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg1),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg2),'MSE: \\u03B1 = {}, \\u03BB = {}'.format(alpha, reg3)])\n",
        "    plt.savefig('Regulation_adjustment_testing_accuracy_LinReg.png')\n",
        "\n",
        "    print_info(loss_train4[-1], accuracy_train4[-1], accuracy_valid4[-1], accuracy_test4[-1], \"GD\", alpha, reg1, end4 - start4)\n",
        "    print_info(loss_train5[-1], accuracy_train5[-1], accuracy_valid5[-1], accuracy_test5[-1], \"GD\", alpha, reg2, end5 - start5)\n",
        "    print_info(loss_train6[-1], accuracy_train6[-1], accuracy_valid6[-1], accuracy_test6[-1], \"GD\", alpha, reg3, end6 - start6)\n",
        "\n",
        "\n",
        "    #1.5\n",
        "    start = time.time()\n",
        "    w_normal_train, b_normal_train = normal_equation(trainData, trainTarget, reg)\n",
        "    end = time.time()\n",
        "    start1 = time.time()\n",
        "    w_normal_train1, b_normal_train1 = normal_equation(trainData, trainTarget, reg1)\n",
        "    end1 = time.time()\n",
        "    start2 = time.time()\n",
        "    w_normal_train2, b_normal_train2 = normal_equation(trainData, trainTarget, reg2)\n",
        "    end2 = time.time()\n",
        "    start3 = time.time()\n",
        "    w_normal_train3, b_normal_train3 = normal_equation(trainData, trainTarget, reg3)\n",
        "    end3 = time.time()\n",
        "    \n",
        "    loss_normal = MSE(w_normal_train, b_normal_train, trainData, trainTarget, reg)\n",
        "    loss_normal1 = MSE(w_normal_train1, b_normal_train1, trainData, trainTarget, reg1)\n",
        "    loss_normal2 = MSE(w_normal_train2, b_normal_train2, trainData, trainTarget, reg2)\n",
        "    loss_normal3 = MSE(w_normal_train3, b_normal_train3, trainData, trainTarget, reg3)\n",
        "    \n",
        "    w_normal_train = [w_normal_train]\n",
        "    w_normal_train1 = [w_normal_train1]\n",
        "    w_normal_train2 = [w_normal_train2]\n",
        "    w_normal_train3 = [w_normal_train3]\n",
        "    b_normal_train = [b_normal_train]\n",
        "    b_normal_train1 = [b_normal_train1]\n",
        "    b_normal_train2 = [b_normal_train2]\n",
        "    b_normal_train3 = [b_normal_train3]\n",
        "\n",
        "    normal_accuracy_train = accuracy_calculation(w_normal_train, b_normal_train, trainData, trainTarget)\n",
        "    normal_accuracy_train1 = accuracy_calculation(w_normal_train1, b_normal_train1, trainData, trainTarget)\n",
        "    normal_accuracy_train2 = accuracy_calculation(w_normal_train2, b_normal_train2, trainData, trainTarget)\n",
        "    normal_accuracy_train3 = accuracy_calculation(w_normal_train3, b_normal_train3, trainData, trainTarget)\n",
        "\n",
        "    normal_accuracy_valid = accuracy_calculation(w_normal_train, b_normal_train, validData, validTarget)\n",
        "    normal_accuracy_valid1 = accuracy_calculation(w_normal_train1, b_normal_train1, validData, validTarget)\n",
        "    normal_accuracy_valid2 = accuracy_calculation(w_normal_train2, b_normal_train2, validData, validTarget)\n",
        "    normal_accuracy_valid3 = accuracy_calculation(w_normal_train3, b_normal_train3, validData, validTarget)\n",
        "\n",
        "    normal_accuracy_test = accuracy_calculation(w_normal_train, b_normal_train, testData, testTarget)\n",
        "    normal_accuracy_test1 = accuracy_calculation(w_normal_train1, b_normal_train1, testData, testTarget)\n",
        "    normal_accuracy_test2 = accuracy_calculation(w_normal_train2, b_normal_train2, testData, testTarget)\n",
        "    normal_accuracy_test3 = accuracy_calculation(w_normal_train3, b_normal_train3, testData, testTarget)\n",
        "\n",
        "    print_info(loss_normal, normal_accuracy_train[0], normal_accuracy_valid[0], normal_accuracy_test[0], \"normal\", 0, reg, end - start)\n",
        "    print_info(loss_normal1, normal_accuracy_train1[0], normal_accuracy_valid1[0], normal_accuracy_test1[0], \"normal\", 0, reg1, end1 - start1)\n",
        "    print_info(loss_normal2, normal_accuracy_train2[0], normal_accuracy_valid2[0], normal_accuracy_test2[0], \"normal\", 0, reg2, end2 - start2)\n",
        "    print_info(loss_normal3, normal_accuracy_train3[0], normal_accuracy_valid3[0], normal_accuracy_test3[0], \"normal\", 0, reg3, end3 - start3)\n",
        "    \n",
        "\n",
        "linreg()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}